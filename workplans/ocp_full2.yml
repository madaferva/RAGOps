# pipeline
---
- name: OCP full 2 workplan 
  hosts: localhost
  gather_facts: false

  vars:

    # General infra

    workplan_name: "ocp_full_2"
    log: "/home/davidfv/experiments/OCP_full2/{{ workplan_name }}.log"
    
    embeddings_url: "http://10.10.78.12:8001/v1/embeddings"
    embeddings_key: "none"
    embeddings_model: "nvidia/nv-embedqa-mistral-7b-v2"
    
    llm_url: "http://10.10.78.11:8079/v1"
    llm_key: "none"
    llm_model: "google/gemma-3-27b-it"
    
    bd_host: "192.168.1.68"
    bd_port: "6333"

    base: "/home/davidfv/experiments/OCP_full2"

    # Chunking
   
    chunks: "{{ base }}/chunks"
    pdfs: "/home/davidfv/datasets/OCP"
    c_size: 400
    c_overlap: 30 
    
    # Embedding
    embeddings: "{{ base }}/embeddings"
    embs_file: "ocp_full_2_embeddings.json"

    # Ingestion
    collection: "ocp_full_2"
    
    # Retrieving
    retriever: "{{ base }}/retriever"
    retriever_file: "ocp_full2_retrievers.json"
    retriever_top_k: "15"

    # Augmentation
    answers: "{{ base }}/answers"
    answers_file: "ocp_full_2_answers.json"
    augmentation_top_k: "15"
    temp: "0.4"
    max_tok: "350"

    # Validation
    validation: "{{ base }}/validation"
    validation_file: "ocp_full_2_validation.json"

  tasks:

    #### CHUNKING

    - name: Chunking Modelo 2
      include_role:
        name: local.rag.chunking
      vars:
        pdf_folder: "{{ pdfs }}"
        output_path: "{{ chunks }}"
        chunk_size: "{{ c_size }}"
        chunk_overlap: "{{ c_overlap }}"
     
    #### INDEXING

    - name: Indexing Modelo 2
      include_role:
        name: local.rag.indexer
      vars:
        api_url: "http://10.10.78.12:8001"
        api_key: "{{ embeddings_key }}"
        txt_folder: "{{ chunks }}"
        output_path: "{{ embeddings }}"
        embeddings_file: "{{ embs_file }}"


    #### INGESTION

    - name: Ingestion Modelo 2
      include_role:
        name: local.rag.ingestion
      vars:
        json_path: "{{ embeddings }}/{{ embs_file }}"
        collection_name: "{{ collection }}"
        host_port: "{{ bd_port}}"
        host: "{{ bd_host }}"

    #### RETRIEVER

    - name: Retriever Modelo 2
      include_role:
        name: local.rag.retriever
      vars:
        input_file: "{{ retriever }}/ocp_full_queries.json"
        api_url: "http://10.10.78.12:8001"
        api_key: "{{ embeddings_key }}"
        model_name: "{{ embeddings_model }}"
        host: "{{ bd_host }}"
        port: "{{ bd_port }}"
        collection_name: "{{ collection }}"
        top_k: "{{ retriever_top_k }}"
        output_dir: "{{ retriever }}"
        output_file: "{{ retriever_file }}"

    #### AUGMENTATION
    
    - name: Augmentation Modelo 2
      include_role:
        name: local.rag.augmentation
      vars:
        json_path: "{{ retriever }}/{{ retriever_file }}"
        output_answer: "{{ answers }}/{{ answers_file }}"
        top_k: "{{ augmentation_top_k }}"
        model_name: "{{ llm_model }}"
        api_url: "{{ llm_url }}"
        api_key: "{{ llm_key }}"
        temperature: "{{ temp }}"
        max_tokens: "{{ max_tok }}"

    #### VALIDATION

    
    - name: Validation Modelo 2
      include_role:
        name: local.rag.validation
      vars:
        answer_file: "{{ answers }}/{{ answers_file }}"
        output_validation: "{{ validation }}/{{ validation_file }}"
        embeddings_api_url: "{{ embeddings_url }}"
        embeddings_api_key: "{{ embeddings_key }}"
        embeddings_model_name: "{{ embeddings_model }}"
        llm_api_url: "{{ llm_url }}"
        llm_api_key: "{{ llm_key }}"
        llm_model_name: "{{ llm_model }}"




  